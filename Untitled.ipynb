{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('window-size=1920x1080')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(\"https://twitter.com/search?f=tweets&vertical=default&q=javascript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = driver.find_elements_by_xpath(\"//li[@data-item-type='tweet']\")\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "tweets = driver.find_elements_by_xpath(\"//li[@data-item-type='tweet']\")\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "def new_posts(driver, min_len):\n",
    "    return len(driver.find_elements_by_xpath(\"//li[@data-item-type='tweet']\")) > min_len\n",
    "\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "WebDriverWait(driver, 10).until(lambda driver: new_posts(driver, len(tweets)))\n",
    "tweets = driver.find_elements_by_xpath(\"//li[@data-item-type='tweet']\")\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "while len(tweets) < 200:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    WebDriverWait(driver, 10).until(lambda driver: new_posts(driver, len(tweets)))\n",
    "\n",
    "    tweets = driver.find_elements_by_xpath(\"//li[@data-item-type='tweet']\")\n",
    "    \n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.save_screenshot(\"current.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "driver.save_screenshot(\"top.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.html as parser\n",
    "\n",
    "html = parser.fromstring(driver.page_source)\n",
    "\n",
    "tweets_lxml = html.xpath(\"//li[@data-item-type='tweet']\")\n",
    "\n",
    "tweet = tweets_lxml[0]\n",
    "fullname = tweet.xpath(\".//*[contains(@class, 'fullname')]/text()\")[0]\n",
    "username = \"\".join(tweet.xpath(\".//*[contains(@class, 'username')]/descendant-or-self::*/text()\"))\n",
    "t = \"\".join(tweet.xpath(\".//*[contains(@class, 'js-tweet-text-container')]/descendant-or-self::*\"\n",
    "                                \"[not(self::*[contains(@class, 'tco-ellipsis')])]/text()\")).strip()\n",
    "extracted_tweet = {\"fullname\": fullname, \"username\": username, \"tweet\": t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fullname': 'Sourabh Parmar',\n",
       " 'username': '@iamsourabhh@NyaBlk@JavaScriptDaily',\n",
       " 'tweet': 'Interesting idea! I have one other idea, I hope you like it.\\n\\nfunction isWaterPokemon(pokemon) {\\n   let waterPokemons = {\\n      squirtell: \"squirtell\",\\n      pokemon2: \"pokemon2\"\\n   };\\n   return waterPokemons[pokemon] !== undefined;\\n};'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u261e' in position 1020: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-8071bfd0c232>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[0mcrawler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTwitterCrawler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m \u001b[0mcrawler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawl_list_and_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"javascript\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"scraping\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrawler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-8071bfd0c232>\u001b[0m in \u001b[0;36mcrawl_list_and_save\u001b[1;34m(self, search_list)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcrawl_list_and_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawl_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_items\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcrawl_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-8071bfd0c232>\u001b[0m in \u001b[0;36msave_items\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mdict_writer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mdict_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriteheader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mdict_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcrawl_list_and_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tgm_t\\appdata\\local\\programs\\python\\python37-32\\Lib\\csv.py\u001b[0m in \u001b[0;36mwriterows\u001b[1;34m(self, rowdicts)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwriterows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrowdicts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dict_to_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrowdicts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;31m# Guard Sniffer's type checking against builds that exclude complex()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tgm_t\\.virtualenvs\\twitter_crawler-c35v8ygs\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\u261e' in position 1020: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import lxml.html as parser\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "TWEETS_XPATH = \"//li[@data-item-type='tweet']\"\n",
    "BASE_URL = \"https://twitter.com/search?f=tweets&vertical=default&q=\"\n",
    "\n",
    "\n",
    "def new_tweets(driver, min_len):\n",
    "    return len(driver.find_elements_by_xpath(TWEETS_XPATH)) > min_len\n",
    "\n",
    "\n",
    "class TwitterCrawler(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('headless')\n",
    "        options.add_argument('window-size=1920x1080')\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "        self.items = []\n",
    "\n",
    "    def save_items(self):\n",
    "        keys = self.items[0].keys()\n",
    "        with open(\"items.csv\", 'w') as f:\n",
    "            dict_writer = csv.DictWriter(f, keys)\n",
    "            dict_writer.writeheader()\n",
    "            dict_writer.writerows(self.items)\n",
    "\n",
    "    def crawl_list_and_save(self, search_list):\n",
    "        self.crawl_list(search_list)\n",
    "        self.save_items()\n",
    "\n",
    "    def crawl_list(self, search_list):\n",
    "        items = []\n",
    "        for term in search_list:\n",
    "            url = BASE_URL + term\n",
    "            tweets = self.crawl_url(url)\n",
    "            image_name = term + \".png\"\n",
    "            self.screenshot(image_name)\n",
    "            items.append({\n",
    "                \"term\": term,\n",
    "                \"tweets\": tweets,\n",
    "                \"image\": image_name\n",
    "            })\n",
    "        self.items = items\n",
    "\n",
    "    def crawl_url(self, url):\n",
    "        self.driver.get(url)\n",
    "        self.get_tweets(100)\n",
    "        return self.parse_tweets()\n",
    "\n",
    "    def get_tweets(self, num_of_tweets):\n",
    "        self.driver_go_to_bottom()\n",
    "        tweets = self.driver.find_elements_by_xpath(TWEETS_XPATH)\n",
    "        while len(tweets) < num_of_tweets:\n",
    "            try:\n",
    "                WebDriverWait(self.driver, 10).until(\n",
    "                    lambda driver: new_tweets(driver, len(tweets)))\n",
    "            except TimeoutException:\n",
    "                # simple exception handling, just move on in case of Timeout\n",
    "                tweets = self.driver.find_elements_by_xpath(TWEETS_XPATH)\n",
    "                break\n",
    "            tweets = self.driver.find_elements_by_xpath(TWEETS_XPATH)\n",
    "        return tweets\n",
    "\n",
    "    def screenshot(self, image_name):\n",
    "        self.driver_go_to_top()\n",
    "        self.driver.save_screenshot(image_name)\n",
    "\n",
    "    def driver_go_to_bottom(self):\n",
    "        self.driver.execute_script(\n",
    "            \"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    def driver_go_to_top(self):\n",
    "        self.driver.execute_script(\n",
    "            \"window.scrollTo(0, 0);\")\n",
    "\n",
    "    def parse_tweets(self):\n",
    "        html = parser.fromstring(self.driver.page_source)\n",
    "        tweets = html.xpath(TWEETS_XPATH)\n",
    "        extracted_tweets = []\n",
    "        for tweet in tweets:\n",
    "            try:\n",
    "                fullname = tweet.xpath(\n",
    "                    \".//*[contains(@class, 'fullname')]/text()\")[0]\n",
    "            except IndexError:\n",
    "                fullname = \"Not Available\"\n",
    "            username = \"\".join(tweet.xpath(\n",
    "                \".//*[contains(@class, 'username')]/descendant-or-self::*\"\n",
    "                \"/text()\"))\n",
    "            tweet = \"\".join(tweet.xpath(\n",
    "                \".//*[contains(@class, 'js-tweet-text-container')]\"\n",
    "                \"/descendant-or-self::*[not(self::*[contains(@class,\"\n",
    "                \" 'tco-ellipsis')])]/text()\"\n",
    "            )).strip()\n",
    "            extracted_tweets.append({\n",
    "                \"fullname\": fullname,\n",
    "                \"username\": username,\n",
    "                \"tweet\": tweet\n",
    "            })\n",
    "        return extracted_tweets\n",
    "\n",
    "\n",
    "crawler = TwitterCrawler()\n",
    "crawler.crawl_list_and_save([\"javascript\", \"python\", \"scraping\"])\n",
    "len(crawler.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
